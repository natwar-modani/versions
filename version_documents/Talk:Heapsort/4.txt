 numerical recipies copyright 
is it acceptable to use code adapted from numerical recipies in c? i don't own a copy, so i can't quote it, but i am led to understand that it is very restrictive in its usage licence.  — philhibbs | talk
that's correct. the code can only be used on one screen if you purchase a single-screen license; distribution is prohibited. deco 00:27, 27 apr 2005 (utc)

 pseudocode question 

i'm fairly certain that the pseudocode in this article does not work on some arrays of odd length.  that is, if length() is odd, then occassionally heapsort() produces an array that is one swap away from sorted.  for example, 
  heapsort([1,2,3]) -> [1,3,2]
and 
  heapsort([2,1,3,5,4]) -> [1,2,4,3,5].
if, in heapsort, i replace the line
    var int start := count / 2 - 1
with
    var int start := count / 2
it seems to correct the bug.
have i missed something?
--quaternion 22:28, 18 feb 2005 (utc)

is it for some reason impractical to implement a quaternary heapsort?

it's not impractical to implement a quaternary heapsort. in fact, i managed to implement an n-ary heapsort algorithm in java. — pguedes

i think there's a bug in the final line of the python program. should it not be called with the first element of the array, 1 instead of with 0?

to my mind o(n log n) is not approximately linear for large n,
but i suppose it depends on your viewpoint. there are sort methods which are linear - e.g pigeon hole sort for a densely populated set of integers, and these
could be expected to run significantly faster.

however, both heapsort and quicksort are, for most practical purposes, amongst the
fastest sorts. note that scrambling the elements before doing a quicksort is often
more effective - as often the elements are almost in order, in which case quicksort is known to be slow. this does not apply to heapsort.

i agree with the o(n log n) statement, and eliminated the claim that it's about linear. scrambling the elements isn't a really effective strategy for improving quicksort, since scrambling involves a lot of cache misses, making it a very slow operation. deco 19:36, 4 nov 2004 (utc)

the article states that an in-place (o(1) auxiliary space) quicksort is possible.  is that true?  don't you need a stack to manage what parts of the array you still need to sort? --ryan stone 15:52, 4 nov 2004 (utc)

oh, i see it now. the data stored on the stack in quicksort is the pivot positions. it is quite possible to compute, rather than store, the pivot positions, in restricted implementations — for example, in a quicksort implementation which always uses the median. i'm not aware of any fast in-place quicksort variant though. deco 19:31, 4 nov 2004 (utc)

there is something i've always wondered about heapsort. when you've created the heap and starts to extract the largest element of it, you swap it with the element with the largest index in the heap to get the value in the right position (ie when you just have created the heap, the first thing you do is to swap the 1st element with the nth, then heapify, then you swap the 1st element with the (n-1)th and heapify and so on). but if you do that you will most of a time get a very low value at the top of the heap (since the value was previously at the bottom of the heap), and since that value is pretty low the siftup will take longer. why don't you instead implement the heap as a min-heap instead of a max-heap? then the head would be at the right position directly (no need to swap) and you could make one of it's children (the element just after it) the head and then heapify. the heapifying would take shorter time on average wouldn't it? i mean, it obvoiusly wouldn't be asymptotically faster, but it would be faster, n'est pas? gkhan 01:21, feb 14, 2005 (utc)
and oh yeah, i realize that the talk-page isn't really the right forum for asking techincal questions, but indulge me gkhan 01:22, feb 14, 2005 (utc)

 note: i removed my previous answer here -- i had the algorithm gkhan was describing completely wrong
 put briefly, the algorithm you describe isn't very effective at all -- assymptotically it's worse than bubble sort.  actually, what you describe is an expensive version of selection sort.
 here's the reason why it's so poor: the heapifying operation takes o(n log n).  what you're suggesting is that we heapify n elements, then n - 1 elements, ... until finally heapifying an array with 1 element(ok, we could skip that, but it doesn't make too much of a difference, and does make the calculations below easier.
 the number of operations we do will thus be o(n log n) + o ( (n - 1) log (n - 1) ) + ... + o ( 1 log 1)
 so it's o ( sum(i = 1 to n) of i log i).  i'm not if that can be simplified, but i do know that sum(i = 1 to n) of i is n(n + 1)/2.  so your algorithm will be worse than o (n ^ 2).
 i don't think that on average, things will be much faster.  let me think about it and see if i can come up with an answer for that.--ryan stone 21:03, 18 feb 2005 (utc)

 well, i've thought about this a bit more, and what i realized was that even if gkhan's algorithm was faster in the average case, it still probably couldn't beat quicksort.  heapsort's greatest advantages over quicksort is its guaranteed o (n log n) and that it's an in-place sort.  if you're willing accept poor performance on certain inputs anyway, you might as well go with quicksort.--ryan stone 02:08, 23 feb 2005 (utc)