# versions
This repository is for sharing the dataset of version detection problem. The dataset is created from Wikipedia revision history.

We took Wikipedia revision dumps that contain all revisions of Wikipedia pages on a certain topic. Each revision is accompaniedby some metadata (like timestamp, parent_id, etc.). To ensure that we consider versions that are sufficiently different from each other, we use timestamps in an automated filtering process to create the dataset.

Each edit made by the user to a Wikipedia page can be considered a separate version of that Wikipedia page (these are referred to as revisions in the Wikipedia terminology). However, oftem there are some artifacts in the revisions â€“ for example, troll edits are insincere edits made by users.We observe that these edits do not last, and the page is reset to the last relevant version by the moderators within a short duration. Based on this observation, we remove all edits that do not last more than ğ´ğ‘‰ğ¸/10 seconds, where ğ´ğ‘‰ğ¸ is defined as the average time between successive edits for a given Wikipedia page. Instead of setting a global threshold, this threshold varies for every page because some pages are edited more frequently than others.

We use timestamps obtained along with metadata for the purpose of filtering. Next, we filter out pages that are rarely edited. This is doneby setting a hard threshold â€“ i.e., pages should consist of at least 10 versions. After this we stochastically sample âŒŠğ‘ˆ (ğ‘šğ‘–ğ‘›,ğ‘¤ğ‘–ğ‘‘ğ‘¡â„)âŒ‹ revisions from all versions, U is the uniform distribution, ğ‘šğ‘–ğ‘› is the minimum number of samples,ğ‘šğ‘–ğ‘› +ğ‘¤ğ‘–ğ‘‘ğ‘¡â„ + 1 is the maximum number of samples, and âŒŠ.âŒ‹ is the greatest integer function. We take ğ‘šğ‘–ğ‘› to be 7 and ğ‘¤ğ‘–ğ‘‘ğ‘¡â„ as 5. However, since it is still possible that the set of revisions obtained via uniform sampling donâ€™t have considerable differences between them, we stochastically filter out more revisions to encourage differences between them. This is done by having higher probability of sampling versions which have more difference in terms of length of the documents with the previously selected versions. This increases the probability of having the set of versions that differ more in content.

Once we obtain the list pages and the versions being considered for each of them, we scrape the Wikipedia article corresponding to each version. We remove all additional information in sections like References, Category, etc. from the scraped articles. This is done to ensure that the model doesnâ€™t utilize this information to learn undesirable patterns, and to ensure that the identification ofrelations is solely based on the content.

Our final dataset comprises 1, 755 unique Wikipedia articles with a total of 10, 267 versions; each article having 5.85 Â± 0.28 versionson average. We split the dataset into train, validation, and test sets in the ratio of 0.7 : 0.1 : 0.2, respectively.
